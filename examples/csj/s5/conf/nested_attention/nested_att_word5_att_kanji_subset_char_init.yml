parent: ./conf/nested_attention/nested_att_word5_att_kanji_subset.yml
param:
  # framework
  backend: pytorch

  # optimization
  pretrain_stage: False

  # initialization
  char_init: /n/sd8/inaguma/result/csj/pytorch/hierarchical_attention/word5_kanji_wb/subset/blstm320H5L4L_drop8_lstm320H1L_adam_lr1e-3_location_dropen0.2de0.2emb0.2_ss0.2_ls0.1_main0.0_input80

  # MTL
  # main_loss_weight: 0.5
  main_loss_weight: 0.8
  curriculum_training: False

    # W2C attention
    usage_dec_sub: update_decoder
    # usage_dec_sub: all
    # usage_dec_sub: no_use

    dec_out_sub_attend_temperature: 1
    dec_out_sub_sigmoid_smoothing: True

    attention_regularization: True

    gate_dec_sub: no_gate
    # gate_dec_sub: scalar
    # gate_dec_sub: elementwise
