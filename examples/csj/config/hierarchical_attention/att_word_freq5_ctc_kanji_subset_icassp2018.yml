parent: ../config/hierarchical_attention/att_word_freq5_att_kanji_subset_baseline.yml
param:
  # framework
  backend: pytorch
  # backend: chainer

  # topology
  encoder_num_layers: 3
  encoder_num_layers_sub: 3
  subsample_list:
    - False
    - False
    - False
  attention_dim: 320

  # optimization
  batch_size: 32

  # regularization
  scheduled_sampling_ramp_max_step: 400000

  # annealing
  print_step: 1000

  # MTL
  main_loss_weight: 0.8
  ctc_loss_weight_sub: 0.2
