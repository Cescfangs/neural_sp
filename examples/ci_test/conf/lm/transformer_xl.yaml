# topology
lm_type: transformer_xl
transformer_d_model: 64
transformer_d_ff: 256
transformer_ffn_activation: relu
n_layers: 2
transformer_n_heads: 4
transformer_param_init: xavier_uniform
tie_embedding: true
# optimization
batch_size: 1
bptt: 10
mem_len: 10
optimizer: noam
n_epochs: 100
convert_to_sgd_epoch: 100
print_step: 1
lr_factor: 1.0
early_stop_patient_n_epochs: 5
eval_start_epoch: 1
warmup_n_steps: 4000
accum_grad_n_steps: 8  ### this is faster
# regularization
clip_grad_norm: 1.0
dropout_in: 0.1
dropout_hidden: 0.0
dropout_out: 0.0
dropout_att: 0.1
dropout_layer: 0.0
weight_decay: 1e-6
lsm_prob: 0.1  ### TODO: confirm if this is effective
adaptive_softmax: false
