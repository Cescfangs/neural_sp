# topology
lm_type: transformer
d_model: 512
d_ff: 2048
n_layers: 6
attn_type: scaled_dot
attn_n_heads: 8
pe_type: add
tie_embedding: true
# optimization
batch_size: 64
bptt: 128
optimizer: adam
learning_rate_factor: 1
n_epochs: 100
convert_to_sgd_epoch: 100
print_step: 200
decay_start_epoch: 10
decay_rate: 0.9
decay_patient_n_epochs: 0
decay_type: epoch
not_improved_patient_n_epochs: 10
eval_start_epoch: 1
warmup_n_steps: 4000
# regularization
clip_grad_norm: 1.0
dropout_hidden: 0.2
dropout_out: 0.0
dropout_emb: 0.2
dropout_att: 0.2
weight_decay: 1e-6
backward: false
adaptive_softmax: false
# contextualization
serialize: true
